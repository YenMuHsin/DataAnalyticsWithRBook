# 資料探勘 {#datamining}

## 什麼是資料探勘
**資料探勘（Data mining）**是用人工智慧、機器學習、統計學的交叉方法，在相對較大型的資料集中發現模式的計算過程。使用資料探勘技術可以建立從**輸入資料**學習新資訊，變成智慧的**演算法**或**資料模式**，用來**預測事件**或**協助決策**。所以，當資料太`少`或`太髒`的時候，資料探勘的效力會被影響。

資料探勘要派上用場，必須有以下條件：

- 有一些模式/模型可`學`
- 很難定義這些模式/模型
- 有資料可`學`這些模式/模型

資料探勘的應用範例如下：

- 天氣預測
- 搜尋建議、購物建議
- 股市預測
- 臉部辨識、指紋辨識
- 垃圾郵件標記
- 尿布啤酒

資料探勘可分為**監督式**學習與**非監督式**學習，監督式學習的特點是訓練資料中有**正確答案**，由輸入物件和預期輸出所組成，而演算法可以由訓練資料中學到或建立一個模式，並依此模式推測新的實例；非監督式學習則不用提供**正確答案**，也就是不需要人力來輸入標籤，單純利用訓練資料的特性，將資料分群分組。

此兩種學習可解決不同的問題，條列如下：

- Supervised learning 監督式學習
    - Regression 迴歸：真實的'值'（股票、氣溫）
    - Classification 分類：分兩類（P/N, Yes/No, M/F, Sick/Not sick）/分多類 (A/B/C/D)

- Unsupervised learning 非監督式學習
    - Clustering 分群
    - Association Rules 關聯式規則

在**監督式**學習中常見的資料探勘演算法如下： 

  - Linear Regression 線性迴歸
  - Logistic Regression 羅吉斯迴歸、邏輯迴歸
  - Support Vector Machines 支持向量機
  - Decision Trees 決策樹
  - K-Nearest Neighbor
  - Neural Networks 神經網路
  - Deep Learning 深度學習


在**非監督式**學習中常見的資料探勘演算法如下： 

  - Hierarchical clustering 階層式分群
  - K-means clustering
  - Neural Networks 神經網路
  - Deep Learning 深度學習

以下介紹在R中使用各類演算法的方法

## Regression 迴歸
Regression Analysis 迴歸分析主要用在了解兩個或多個變數間`是否相關`、`相關方向與強度`，並建立`數學模型`以便觀察特定變數來預測研究者感興趣的變數，常見的迴歸分析演算法包括：

- Linear Regression 線性迴歸
- Logistic Regression 羅吉斯迴歸、邏輯迴歸

### Linear Regression 線性迴歸

首先，嘗試將Linear Regression 線性迴歸用在NBA的資料看看，做NBA`得分`與`上場分鐘數`的線性迴歸觀察
```{r linear1,message=FALSE,warning=FALSE}
#讀入SportsAnalytics package
library(SportsAnalytics)
#擷取2015-2016年球季球員資料
NBA1516<-fetch_NBAPlayerStatistics("15-16")
```

```{r linear2,warning=F,message=F,fig.height=4}
library(ggplot2)
ggplot(NBA1516,aes(x=TotalMinutesPlayed,y=TotalPoints))+
    geom_point()+geom_smooth(method = "glm")
```

在R中，最基本的簡單線性迴歸分析為`lm()`，使用方法為`lm(formula,data=資料名稱)`，搭配formula使用，formula的撰寫方法為：依變項~自變項1＋自變項2＋....
```{r linear3,warning=F,message=F,fig.height=4.5}
lm(TotalPoints~TotalMinutesPlayed,data =NBA1516)
```
由此可知總得分數TotalPoints等於`0.4931` * 總出場分鐘數 `-85.9071`

TotalPoints = `0.4931` * TotalMinutesPlayed `-85.9071`

更被廣泛使用的是廣義線性迴歸模型generalized linear models (glm)，函數為`glm()`，使用方法與`lm()`類似，包括了線性迴歸模型和邏輯迴歸模型。
如果需要修改預設模型，可設定family參數：

    - `family="gaussian"` 線性模型模型
    - `family="binomial"` 邏輯迴歸模型
    - `family="poisson"` 卜瓦松迴歸模型

Gaussian distribution高斯函數是`常態分布`的密度函數

Binomial distribution二項分布是`n個獨立的是/非試驗中成功的次數`的離散機率分布

Poisson distribution`次數`分佈：

- 某一服務設施在一定時間內受到的服務請求的次數
- 公車站的候客人數
- 機器故障數
- 自然災害發生的次數
- DNA序列的變異數.....

以下為使用多變量線性迴歸來分析`得分`與`上場分鐘數`和`兩分球出手數`的關係範例

```{r linear4,warning=F,message=F,fig.height=4.5}
# e+01: 10^1 / e-04: 10^(-4)
glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted,
    data =NBA1516)
```
由此可知總得分數等於`-0.0002347` * 總出場分鐘數 + `1.255794` * 兩分球出手數  `-17.99`
TotalPoints = `-0.0002347` * TotalMinutesPlayed + `1.255794` * FieldGoalsAttempted  `-17.99`


如需使用多變量線性迴歸來分析`得分`與`上場分鐘數`和`兩分球出手數`和`守備位置`的關係，可修改formula

```{r linear5,warning=F,message=F,fig.height=4.5}
glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted+Position,
    data =NBA1516)
# e+01: 10^1 / e-04: 10^(-4)
```
由此可知總得分數TotalPoints和`上場分鐘數`和`兩分球出手數`和`守備位置`的關係為：
TotalPoints = `-0.0065` * TotalMinutesPlayed + `1.28` *FieldGoalsAttempted  `+22.85` + `22.85` * PositionPF + `-65.03` * PositionPG + `-38.52` * PositionSF + `-52.18` * PositionSG

由上述結果可發現，`守備位置`的變項被轉為**虛擬變項 Dummy Variable**：PositionPF、PositionPG、PositionSF、PositionSG，如果是控球後衛（PG），會得到：

  - PositionPF=0
  - PositionPG=1
  - PositionSF=0
  - PositionSG=0

可能有人會問，那中鋒去哪了？其實中鋒被當作基準項，也就是當守備位置是中鋒(C)時，會得到：

  - PositionPF=0
  - PositionPG=0
  - PositionSF=0
  - PositionSG=0

總結以上，多變量線性迴歸分析有下列特色：

- 假設：各變數相互獨立！
- 若自變項X是類別變項，需要建立`虛擬變項`
- 在R裡，`類別變項`請記得轉成factor，R會自動建立`虛擬變項`
- 用在`依變數為連續變數`，`自變數為連續變數或虛擬變數`的場合


### Logistic Regression 羅吉斯迴歸


Logistic Regression 羅吉斯迴歸常用在`依變數為二元變數（非0即1）`的場合，如：
  - 生病/沒生病
  - 錄取/不錄取
  - `family="binomial"` 邏輯迴歸模型

以分數資料為例，分析為什麼錄取/不錄取？
```{r logistic1,warning=F,message=F,fig.height=4.5}
mydata <- read.csv("https://raw.githubusercontent.com/CGUIM-BigDataAnalysis/BigDataCGUIM/master/binary.csv")
```
```{r warning=F,message=F,eval=F}
# GRE:某考試成績, GPA:在校平均成績, rank:學校聲望
head(mydata)
```
```{r warning=F,message=F,echo=F}
knitr::kable(head(mydata)) 
```

```{r logistic2,warning=F,message=F,fig.height=4.5}
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank,
               data = mydata, family = "binomial")
sum<-summary(mylogit)
sum$coefficients
```

### 最佳模型篩選
到底該用哪個模型來預測，會得到最準確的結果？在迴歸模型中，常用的判斷準則包括：

  - Akaike’s Information Criterion (AIC)
  - Bayesian Information Criterion (BIC)
    
AIC和BIC都是數值越小越好，以下建立三個模型，並比較其AIC，
```{r linear7,warning=F,message=F,fig.height=4.5}
OneVar<-glm(TotalPoints~TotalMinutesPlayed,data =NBA1516)
TwoVar<-glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted,
            data =NBA1516)
ThreeVar<-glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted+Position,
              data =NBA1516)
c(OneVar$aic,TwoVar$aic,ThreeVar$aic)
```

在建立迴歸模型時，常會遇到到底該放多少參數？所有參數都有用嗎？這類的問題，我們可以藉由觀察coefficients來判斷參數在模型中的"實用程度"
```{r linear8,warning=F,message=F,fig.height=4.5}
sum2<-summary(TwoVar)
sum2$coefficients
```

```{r linear9,warning=F,message=F,fig.height=4.5}
sum3<-summary(ThreeVar)
sum3$coefficients
```

## Decision Trees 決策樹

決策樹是在`樹狀目錄`中建立一系列分割，以建立模型。這些分割會表示成`「節點」(Node)`。每次發現輸入資料行與可預測資料行有明顯地相互關聯時，此演算法就會在模型中加入一個`節點`。演算法決定分岔的方式不同，視它預測連續資料行或分隔資料行而定。

```{r echo=F}
library(rpart)
library(rpart.plot)
DT<-rpart(Position~Blocks+ThreesMade+Assists+Steals,data=NBA1516)
prp(DT)	
```

以下介紹常見的Classification And Regression Tree (CART)，使用前須先安裝`rpart` packages [@R-rpart]

```{r rpart1,eval=F,warning=F,message=F,fig.height=4.5}
install.packages("rpart")
```
以前述NBA資料為例，嘗試用用籃板/三分/助攻/抄截數據來判斷守備位置，建立決策樹的函數為`rpart()`，使用方式為`rpart(formula, data)`
```{r rpart2,warning=F,message=F,fig.height=4.5}
library(rpart)
DT<-rpart(Position~Blocks+ThreesMade+Assists+Steals,data=NBA1516)
DT
#控球後衛（PG）、得分後衛（SG）、小前鋒（SF）、大前鋒（PF）和中鋒（C）
```

```{r rpart3,warning=F,message=F,fig.height=4.5}
par(mfrow=c(1,1), mar = rep(1,4)) #下,左,上,右
plot(DT)
text(DT, use.n=F, all=F, cex=1)
#控球後衛（PG）、得分後衛（SG）、小前鋒（SF）、大前鋒（PF）和中鋒（C）
```

可以看出預設的`plot()`畫出來的圖很難看懂，可以改用`rpart.plot` package [@R-rpart.plot]裡面的`prp()`
```{r rpart41,eval=F,warning=F,message=F,fig.height=4.5}
install.packages("rpart.plot") #第一次使用前須先安裝
```
```{r rpart4,warning=F,message=F,fig.height=4.5}
library(rpart.plot)
prp(DT)	
```

決策樹演算法決定`節點`的方式如下：

- Gini impurity
- Information gain
- Variance reduction

細節可參考[維基百科](https://en.wikipedia.org/wiki/Decision_tree_learning)


## Clustering 分群
Clustering 分群的目的是將相近的觀察值作做分群，分群過程中，可能會遇到以下問題：

- 如何定義相近？
- 如何分群？
- 如何視覺化？
- 如何解釋分群？

### Hierarchical clustering 階層式分群

- An agglomerative approach
    - Find closest two things
    - Put them together
    - Find next closest

- Requires
    - A defined distance
    - A merging approach

- Produces
    - A tree showing how close things are to each other

如何定義相近？用距離`distance`的概念來定義相近。

- Distance or similarity
    - Continuous - euclidean distance
    - Continuous - correlation similarity
    - Binary - manhattan distance

- Pick a distance/similarity that makes sense for your problem


 Example distances - Euclidean

$$\sqrt{(A_1-A_2)^2 + (B_1-B_2)^2 + \ldots + (Z_1-Z_2)^2}$$

 Example distances - Manhattan

$$|A_1-A_2| + |B_1-B_2| + \ldots + |Z_1-Z_2|$$

 Merging apporach
 
- Agglomerative 聚合

    - Single-linkage：取最小值
    - Complete-linkage：取最大值
    - Average-linkage：取平均值
    

 Hierarchical clustering - hp vs. mpg
```{r, echo=F, fig.height=5.5,fig.width=8}
par(mfrow=c(1,1),mar=rep(2,4))
plot(mtcars$hp, mtcars$mpg, col = "blue", pch = 19, cex = 2)
text(mtcars$hp + 25, mtcars$mpg,
     labels = as.character(rownames(mtcars)))
```

Hierarchical clustering - #1

```{r echo=F,message=F,warning=F, fig.height=5,fig.width=8}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
labelCar<-rownames(mtcars)[-1]
#install.packages("fields")
library(fields)
dataFrame <- data.frame(x=x,y=y)
rdistxy <- rdist(dataFrame) #Euclidean distance
diag(rdistxy) <- diag(rdistxy) + 1e5

# Find the index of the points with minimum distance
ind <- which(rdistxy == min(rdistxy),arr.ind=TRUE)
par(mfrow=c(1,2),mar=rep(1,4))
plot(x,y,col="blue",pch=19,cex=1)
text(x+0.05,y+0.05,labels=labelCar)
points(x[ind[1,]],y[ind[1,]],col="orange",pch=19,cex=1)

# Make a cluster and cut it at the right height
distxy <- dist(dataFrame)
hcluster <- hclust(distxy)
dendro <- as.dendrogram(hcluster)
cutDendro <- cut(dendro,h=(hcluster$height[1]+0.00001) )
plot(cutDendro$lower[[23]],yaxt="n")
```

 Hierarchical clustering - #2
```{r echo=F, fig.height=6,fig.width=8}
# Find the index of the points with minimum distance
ind3 <- which(rdistxy == rdistxy[order(rdistxy)][3],arr.ind=TRUE)
par(mfrow=c(1,1),mar=rep(0.2,4))
# Plot the points with the minimum overlayed
plot(x,y,col="blue",pch=19,cex=1)
text(x+0.05,y+0.05,labels=labelCar)
points(x[ind[1,]],y[ind[1,]],col="orange",pch=19,cex=1)
points(x[ind3[1,]],y[ind3[1,]],col="red",pch=19,cex=1)
```

 Hierarchical clustering - #3

```{r echo=F,fig.height=5,fig.width=8}
# Find the index of the points with minimum distance
ind3 <- which(rdistxy == rdistxy[order(rdistxy)][3],arr.ind=TRUE)
par(mfrow=c(1,3),mar=rep(0.2,4))
# Plot the points with the minimum overlayed
plot(x,y,col="blue",pch=19,cex=1)
text(x+0.05,y+0.05,labels=labelCar)
points(x[ind[1,]],y[ind[1,]],col="orange",pch=19,cex=1)
points(x[ind3[1,]],y[ind3[1,]],col="red",pch=19,cex=1)

# Make dendogram plots
distxy <- dist(dataFrame)
hcluster <- hclust(distxy)
dendro <- as.dendrogram(hcluster)
cutDendro <- cut(dendro,h=(hcluster$height[2]) )
plot(cutDendro$lower[[19]],yaxt="n")
plot(cutDendro$lower[[22]],yaxt="n")

```

可用`dist()`函數計算距離，使用method=""設定計算距離的依據
```{r,fig.height=4,fig.width=6,warning=F,message=F}
mtcars.mxs<-as.matrix(mtcars)
d<-dist(mtcars.mxs) #預設為euclidean
head(d)
```

`dist()`函數可用方法包括：
 "euclidean", "maximum", "manhattan", "canberra", "binary" or "minkowski"
```{r,fig.height=4,fig.width=6,warning=F,message=F}
d<-dist(mtcars.mxs, method="manhattan") #計算manhattan距離
head(d)
```


用`hclust`函數畫圖，必要參數是各觀察值的距離（可用`dist()`函數計算）
```{r,fig.height=4,fig.width=6}
par(mar=rep(2,4),mfrow=c(1,1))
hc<-hclust(dist(mtcars.mxs)) #可用method參數設定聚合方法，預設為complete
plot(hc)
```

```{r,fig.height=4,fig.width=6}
par(mar=rep(2,4),mfrow=c(1,1))
hc<-hclust(dist(mtcars.mxs),method="average") #聚合方法為計算平均距離
plot(hc)
```

```{r,fig.height=4,fig.width=6}
clusterCut <- cutree(hc, k=5) #分5群
sort(clusterCut)
```

```{r,fig.height=4,fig.width=6}
ggplot()+geom_point(data=mtcars,
                    aes(x=hp,y=mpg,color=as.factor(clusterCut)))
```


```{r,fig.height=4,fig.width=6}
clusterCut <- cutree(hc,h =4) #切在高度=4的地方（距離=4）
sort(clusterCut)
```


```{r,fig.height=4.5,fig.width=6}
par(mar=rep(0.2,4),mfrow=c(1,1))
heatmap(mtcars.mxs)
```

```{r, fig.height=4,fig.width=4}
distxy <- dist(mtcars.mxs)
hClustering <- hclust(distxy)
plot(hClustering)
```


 Hierarchical clustering: summary
- 可快速瀏覽觀察值與各欄位的關係

- 分群結果可能被以下參數影響：
    - 計算距離的方法
    - 分群依據
    - 更改數值的大小

- 可能會遇到的問題：
    - 有時會不太清楚要如何切割分群結果


### K-means clustering

- 執行步驟
    - 指定要分幾群
    - 計算每一群的中心點
    - 將各個物件/觀察值指定給最近的中心點
    - 依照新的分群計算新的中心點

- 輸入
    - 計算距離的資料（數值）
    - 要分成幾群 # of clusters
    - 預設個群的中間點位置


- 產出
    - 計算出每’群‘的中心點
    - 指定每個觀察值所在的’群‘


```{r, fig.height=4,fig.width=6}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=labelCar)
```


```{r,echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=labelCar)
cx <- c(-1,0.5,2.5)
cy <- c(2,0,-1)
points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
```


```{r,echo=FALSE,fig.height=4.5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=labelCar)
cx <- c(-1,0.5,2.5)
cy <- c(2,0,-1)
points(cx,cy,col=cols1,pch=3,cex=2,lwd=2)
## Find the closest centroid
distTmp <- matrix(NA,nrow=3,ncol=length(x))
distTmp[1,] <- (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] <- (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] <- (x-cx[3])^2 + (y-cy[3])^2
newClust <- apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
```



```{r,echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=labelCar)

## Find the closest centroid
points(x,y,pch=19,cex=2,col=cols1[newClust])
newCx <- tapply(x,newClust,mean)
newCy <- tapply(y,newClust,mean)
points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)
```



```{r,echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=labelCar)

points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)

## Iteration 2
distTmp <- matrix(NA,nrow=3,ncol=length(x))
distTmp[1,] <- (x-newCx[1])^2 + (y-newCy[1])^2
distTmp[2,] <- (x-newCx[2])^2 + (y-newCy[2])^2
distTmp[3,] <- (x-newCx[3])^2 + (y-newCy[3])^2
newClust2 <- apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust2])

```



```{r,echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 <- c("red","orange","purple")
text(x+0.05,y+0.05,labels=labelCar)

## Final centroids
finalCx <- tapply(x,newClust2,mean)
finalCy <- tapply(y,newClust2,mean)
points(finalCx,finalCy,col=cols1,pch=3,cex=2,lwd=2)
points(x,y,pch=19,cex=2,col=cols1[newClust2])

```




 `kmeans()`

- Important parameters: `x`, `centers`, `iter.max`, `nstart`

```{r kmeans}
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
names(kmeansObj)
kmeansObj$cluster
```



```{r,fig.height=4,fig.width=4}
par(mar=rep(0.2,4))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```



 Heatmaps

```{r,fig.height=3,fig.width=8}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj <- kmeans(dataMatrix,centers=3)
par(mfrow=c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[,nrow(dataMatrix):1],yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)],yaxt="n")
```


 K-means注意事項

- 需要決定# of clusters
    - 用眼睛/人工/特殊要求選
    - 用 cross validation/information theory選
    - [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)


- K-means 沒有一定的結果
    - 不同的 # of clusters
    - 不同的 # of iterations



`kmeans()`, k=2

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=2)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:2,pch=3,cex=3,lwd=3)
```

 `kmeans()`, k=3

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=3)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:3,pch=3,cex=3,lwd=3)
```

`kmeans()`, k=4

```{r,echo=F,fig.height=5,fig.width=4}
x<-scale(mtcars$hp[-1]);y<-scale(mtcars$mpg[-1])
dataFrame <- data.frame(x,y)
kmeansObj <- kmeans(dataFrame,centers=4)
par(mar=rep(0.2,4),mfrow=c(1,1))
plot(x,y,col=kmeansObj$cluster,pch=19,cex=2)
points(kmeansObj$centers,col=1:4,pch=3,cex=3,lwd=3)
```

## Association Rules 關聯式規則

**關聯式規則**用於從大量數據中挖掘出有價值的數據項之間的相關關係，原則為不考慮項目的次序，而僅考慮其組合。著名的`購物籃分析 (Market Basket Analysis)`即為關聯式規則分析的應用。而**Apriori演算法**是挖掘`布林關聯規則` (Boolean association rules) 頻繁項集的算法，在R中，可以使用`arules`[@R-arules] 套件來執行關聯式規則分析。

以下以超市資料為例，使用關聯式規則分析執行購物籃分析。

首先先讀入超市消費資料

```{r warning=F,message=F,fig.height=4.5}
# Load the libraries
if (!require('arules')){
  install.packages("arules");
  library(arules) #for Apriori演算法
}
if (!require('datasets')){
  install.packages("datasets");
  library(datasets) #for Groceries data
}
data(Groceries) # Load the data set
Groceries@data@Dim #169 種商品，9835筆交易資料
```

超市資料的原始樣貌為：
```{r echo=FALSE}
knitr::include_graphics("figure/groceries.png")
```

可使用arules套件中的apriori函數來實作apriori演算法
```{r warning=F,message=F,fig.height=4.5}
# Get the rules
rules <- apriori(Groceries, # data= Groceries
                 parameter = list(supp = 0.001, conf = 0.8), #參數最低限度
                 control = list(verbose=F)) #不要顯示output
options(digits=2) # Only 2 digits
inspect(rules[1:5]) # Show the top 5 rules
```


根據計算結果，解讀模型的方法如下：

啤酒=>尿布

- `Support`: 一次交易中，包括規則內的物品的機率。買啤酒同時買尿布的機率。交集
- `Confidence`: 包含左邊物品A的交易也會包含右邊物品B的條件機率。在買了啤酒的顧客中，有買尿布的比例。
- `Lift`: 規則的信心比期望值高多少。（買了啤酒以後，有買尿布的機率）/（在所有顧客群中買尿布的機率）
    - `lift`=1: items on the left and right are independent.

可用排序功能排序後，列出最有關連（confidence最高）的幾條規則
```{r warning=F,message=F,fig.height=4.5}
rules<-sort(rules, by="confidence", decreasing=TRUE) #按照confidence排序
inspect(rules[1:5]) # Show the top 5 rules
```

特別針對某項商品（右側變數），像是：買了什麼東西的人，會買`牛奶`呢？
```{r warning=F,message=F,fig.height=4.5}
rulesR<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.08),
        appearance = list(default="lhs",rhs="whole milk"), #設定右邊一定要是牛奶
        control = list(verbose=F)) #不要顯示output
rulesR<-sort(rulesR, decreasing=TRUE,by="confidence") #按照confidence排序
inspect(rulesR[1:5]) # Show the top 5 rules
```

特別針對某項商品（左側變數），像是：買了`牛奶`的人，會買什麼呢？
```{r warning=F,message=F,fig.height=4.5}
rulesL<-apriori(data=Groceries, parameter=list(supp=0.001,conf = 0.15,minlen=2),
        appearance = list(default="rhs",lhs="whole milk"), #設定左邊一定要是牛奶
        control = list(verbose=F)) #不要顯示output
rulesL<-sort(rulesL, decreasing=TRUE,by="confidence") #按照confidence排序
inspect(rulesL[1:5]) # Show the top 5 rules
```


 規則視覺化
```{r eval=F,warning=F,message=F,fig.height=4.5}
if (!require('arulesViz')){
  install.packages("arulesViz"); 
  library(arulesViz)
}
#Mac->http://planspace.org/2013/01/17/fix-r-tcltk-dependency-problem-on-mac/
plot(rules,method="graph",interactive=TRUE,shading=NA) #會跑一陣子
```

```{r echo=FALSE}
knitr::include_graphics("figure/arulesViz.png")
```
```{r echo=FALSE}
knitr::include_graphics("figure/arulesVizBig.png")
```

## Open Source Packages
### Prophet

Prophet 是 Facebook在2017年開放出來的時序性預測演算法，用來預測各類資料的時序變化，像是顧客造訪數、溫度、疾病發生率等等，以下是Prophet for R的安裝使用範例

- C/C++ Tool
  - [R Tools](https://cran.r-project.org/bin/windows/Rtools/) on Windows
  - [Command Line Tools](http://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/) on OS X

```{r eval=F}
install.packages('prophet')
```

[R API](https://facebookincubator.github.io/prophet/docs/quick_start.html#r-api)

```{r eval=F}
library(prophet)
library(dplyr)
df <- read.csv('https://raw.githubusercontent.com/facebookincubator/prophet/master/examples/example_wp_peyton_manning.csv') %>%
    mutate(y = log(y))
m <- prophet(df)
future <- make_future_dataframe(m, periods = 365)
tail(future)
forecast <- predict(m, future)
tail(forecast[c('ds', 'yhat', 'yhat_lower', 'yhat_upper')])
plot(m, forecast)
prophet_plot_components(m, forecast)

```


[Prophet官網](https://facebookincubator.github.io/prophet/)

### TensorFlow
- Python 3.5.3 **64 bit** [網站](https://www.python.org/downloads/release/python-353/)
  - Windows x86-64 executable installer
- TensorFlow 1.0.1 [網站](https://www.tensorflow.org/install/)
  - pip3 install --upgrade tensorflow
  - pip3 install --upgrade tensorflow-gpu
- C/C++ Tool
  - [R Tools](https://cran.r-project.org/bin/windows/Rtools/) on Windows
  - [Command Line Tools](http://osxdaily.com/2014/02/12/install-command-line-tools-mac-os-x/) on OS X
- tensorflow package for R [網站](https://rstudio.github.io/tensorflow/index.html)

```{r eval=F}
devtools::install_github("rstudio/tensorflow")
```

TensorFlow for R

- Locating TensorFlow (optional)
- Hello World

```{r eval=F}
library(tensorflow)
sess = tf$Session()
hello <- tf$constant('Hello, TensorFlow!')
sess$run(hello)
```


### MXNet

Amazon
[Install MXNet for R](http://mxnet.io/get_started/windows_setup.html#install-mxnet-for-r)
MXNet for R [Tutorials](http://mxnet.io/tutorials/index.html#r)

MXNet for R
```{r eval=F}
install.packages("drat", repos="https://cran.rstudio.com")
drat:::addRepo("dmlc")
install.packages("mxnet")
```

## 模型驗證
在完成模型訓練後，為了驗證模型訓練的好不好，需要用一組**獨立**的測試資料，來做模型的驗證。所以，在訓練模型前，必須特別留意是否有保留一份**獨立的資料**，並確保在訓練模型時都不用到此獨立資料集。因此，資料集可分為以下兩種：

- **訓練組** Training set, Development set: 讓演算法`學`到`知識`
- **測試組** Test set, Validation set: 驗證`學`的怎麼樣

Training set和Test set通常會比例分配，如2/3的資料設為`Training set`，剩下的1/3做驗證`Test set`。以下圖的監督式學習流程圖為例，可以注意到綠色箭頭的資料集在訓練過程中從未被使用。


```{r echo=FALSE}
knitr::include_graphics("figure/SupervisedLearning.png")
```

### Regression 迴歸驗證

以NBA資料為例，首先先將資料讀入
```{r message=FALSE,warning=FALSE}
#讀入SportsAnalytics package
if (!require('SportsAnalytics')){
    install.packages("SportsAnalytics")
    library(SportsAnalytics)
}
#擷取2015-2016年球季球員資料
NBA1516<-fetch_NBAPlayerStatistics("15-16")
NBA1516<-NBA1516[complete.cases(NBA1516),]
```

- 以Training set來`選看起來最好的模型`
- 用Test set來`驗證模型是不是真的很好`
- 想像.....訓練出來題庫答得好的學生，寫到新題目不一定會寫！？
- 訓練模型時，只能看Training set，用Training set來選一個最好的模型
- 訓練模型時，不能偷看Test set，才是真正的驗證

為分出訓練組與測試組，需使用隨機抽樣的方式
```{r message=FALSE,warning=FALSE}
sample(1:10,3) # 從1到10，隨機取三個數字
sample(1:nrow(NBA1516),nrow(NBA1516)/3) #從第一行到最後一行，隨機取1/3行數
```

使用上述方法，選出1/3的元素位置，把NBA的資料分成Training 和 Test set
```{r message=FALSE,warning=FALSE}
NBA1516$Test<-F #新增一個參數紀錄分組
#隨機取1/3當Test set
NBA1516[sample(1:nrow(NBA1516),nrow(NBA1516)/3),"Test"]<-T
# Training set : Test set球員數
c(sum(NBA1516$Test==F),sum(NBA1516$Test==T))
```


並用訓練組的資料（NBA1516$Test==F），訓練一個多變數線性迴歸模型
```{r warning=F,message=F,fig.height=4.5}
fit<-glm(TotalPoints~TotalMinutesPlayed+FieldGoalsAttempted+
             Position+ThreesAttempted+FreeThrowsAttempted,
              data =NBA1516[NBA1516$Test==F,])
summary(fit)$coefficients
```

逐步選擇模型 stepwise 後退學習：一開始先將所有參數加到模型裡，再一個一個拿掉
```{r warning=F,message=F,fig.height=4.5}
library(MASS)
##根據AIC，做逐步選擇, 預設倒退學習 direction = "backward"
##trace=FALSE: 不要顯示步驟
finalModel_B<-stepAIC(fit,direction = "backward",trace=FALSE)
summary(finalModel_B)$coefficients
```

逐步選擇模型 stepwise 往前學習：一開始先做一個沒有參數的模型，再把參數一個一個加進去
```{r warning=F,message=F,fig.height=4.5}
##根據AIC，做逐步選擇, 往前學習 direction = "forward"
finalModel_F<-stepAIC(fit,direction = "forward",trace=FALSE)
summary(finalModel_F)$coefficients
```

逐步選擇模型 stepwise 雙向學習：參數加加減減
```{r warning=F,message=F,fig.height=4.5}
##根據AIC，做逐步選擇, 雙向學習 direction = "both"
finalModel_Both<-stepAIC(fit,direction = "both",trace=FALSE)
summary(finalModel_Both)$coefficients
```


用Test set來評估模型好不好，使用predict函數，將測試組資料放入預測模型中，預測測試組的結果
```{r warning=F,message=F,fig.height=3}
predictPoint<-predict(finalModel_Both, #Test==T, test data
                      newdata = NBA1516[NBA1516$Test==T,])
cor(x=predictPoint,y=NBA1516[NBA1516$Test==T,]$TotalPoints) #相關係數
plot(x=predictPoint,y=NBA1516[NBA1516$Test==T,]$TotalPoints)
```

### Logistic Regression 邏輯迴歸驗證

首先，先把入學資料分成Training 和 Test set。這邊要特別留意，當答案有正反兩面時，`Level 2 要放正面答案`-->有病/錄取...
```{r warning=F,message=F,fig.height=4.5}
mydata <- read.csv("https://raw.githubusercontent.com/CGUIM-BigDataAnalysis/BigDataCGUIM/master/binary.csv")
mydata$admit <- factor(mydata$admit) # 類別變項要轉為factor
mydata$rank <- factor(mydata$rank) # 類別變項要轉為factor
mydata$Test<-F #新增一個參數紀錄分組
mydata[sample(1:nrow(mydata),nrow(mydata)/3),"Test"]<-T #隨機取1/3當Test set
c(sum(mydata$Test==F),sum(mydata$Test==T)) # Training set : Test set學生數
#修改一下factor的level: 改成Level 2為錄取，1為不錄取-->Level 2 要放正面答案
mydata$admit<-factor(mydata$admit,levels=c(0,1))
```

逐步選擇最好的模型
```{r warning=F,message=F,fig.height=4.5}
# GRE:某考試成績, GPA:在校平均成績, rank:學校聲望
mylogit <- glm(admit ~ gre + gpa + rank,
               data = mydata[mydata$Test==F,], family = "binomial")
finalFit<-stepAIC(mylogit,direction = "both",trace=FALSE) # 雙向逐步選擇模型
summary(finalFit)
```

用預測組預測新學生可不可以錄取，並驗證答案
```{r warning=F,message=F,fig.height=4.5}
AdmitProb<-predict(finalFit, # 用Training set做的模型
                   newdata = mydata[mydata$Test==T,], #Test==T, test data
                   type="response") #結果為每個人被錄取的機率
head(AdmitProb)
table(AdmitProb>0.5,mydata[mydata$Test==T,]$admit) # row,column
```

當答案是二元時：效能指標

- Sensitivity 敏感性
- Specificity 特異性
- Positive Predictive Value (PPV) 陽性預測值
- Negative Predictive Value (NPV) 陰性預測值

名詞解釋

```{r echo=FALSE}
knitr::include_graphics("figure/Cond.png")
```

- TP: 有病且預測也有病
- TN: 沒病且預測也沒病
- FP: 沒病但是預測有病
- FN: 有病但預測沒病


```{r echo=FALSE}
knitr::include_graphics("figure/para.png")
```

當答案是二元時：效能指標公式
 
- Sensitivity 敏感性：所有`真的有病`的人，被`預測有病`的比例
- Specificity 特異性：所有`真的沒病`的人，被`預測沒病`的比例
- Positive Predictive Value (PPV) 陽性預測值：所有被`預測有病`的人，`真的有病`的比例
- Negative Predictive Value (NPV) 陰性預測值：所有被`預測沒病`的人，`真的沒病`的比例

 回想一下剛剛的驗證結果
```{r warning=F,message=F,fig.height=4.5}
table(AdmitProb>0.5,mydata[mydata$Test==T,]$admit) # row,column
```
```{r echo=FALSE}
knitr::include_graphics("figure/para.png")
```


 計算預測效能參數
```{r warning=F,message=F,fig.height=4.5}
AdmitProb<-predict(finalFit,
                   newdata = mydata[mydata$Test==T,], #Test==T, test data
                   type="response") #結果為每個人被錄取的機率
AdmitAns<-factor(ifelse(AdmitProb>0.5,1,0),levels=c(0,1))
str(AdmitAns)
```

 計算預測效能參數
```{r warning=F,message=F,fig.height=4.5}
library(caret) # install.packages("caret") #計算參數的packages
sensitivity(AdmitAns,mydata[mydata$Test==T,]$admit,positive = "1")
specificity(AdmitAns,mydata[mydata$Test==T,]$admit,negative = "0")
posPredValue(AdmitAns,mydata[mydata$Test==T,]$admit,positive = "1")
negPredValue(AdmitAns,mydata[mydata$Test==T,]$admit,negative = "0")
```


### Decision Trees 決策樹驗證

 阻攻/籃板/三分/助攻/抄截判斷位置-訓練
```{r warning=F,message=F,fig.height=4.5}
if (!require('rpart')){
    install.packages("rpart"); library(rpart)
}
DT<-rpart(Position~Blocks+TotalRebounds+ThreesMade+Assists+Steals,
          data=NBA1516[NBA1516$Test==F,]) #訓練組 Training set
#控球後衛（PG）、得分後衛（SG）、小前鋒（SF）、大前鋒（PF）和中鋒（C）
DT
```

阻攻/籃板/三分/助攻/抄截判斷位置-訓練

預設的`plot()`真的太難用，改用`rpart.plot` package的`prp()`
```{r warning=F,message=F,fig.height=4.5}
if (!require('rpart.plot')){
  install.packages("rpart.plot"); 
  library(rpart.plot)
}
prp(DT)	# 把決策樹畫出來
```

 阻攻/籃板/三分/助攻/抄截判斷位置-訓練
```{r warning=F,message=F,fig.height=5}
prp(DT)
```

 有批球員沒寫守備位置？--預測
```{r warning=F,message=F,fig.height=5}
posPred<-predict(DT,newdata= NBA1516[NBA1516$Test==T,]) #Test==T, test data
# 預設為class probabilities, type = "prob"
head(posPred)
```


 有個人沒寫守備位置--對答案
```{r warning=F,message=F,fig.height=5}
result<-cbind(round(posPred,digits = 2),
              NBA1516[NBA1516$Test==T,]$Name,
      as.character(NBA1516[NBA1516$Test==T,]$Position))
head(result)
```


 有個人沒寫守備位置--預測-2
```{r warning=F,message=F,fig.height=5}
posPredC<-predict(DT,newdata= NBA1516[NBA1516$Test==T,],type = "class")
# type = "class" 直接預測類別
head(posPredC)
```

 有個人沒寫守備位置--對答案-2
```{r warning=F,message=F,fig.height=5}
resultC<-cbind(as.character(posPredC),NBA1516[NBA1516$Test==T,]$Name,
      as.character(NBA1516[NBA1516$Test==T,]$Position))
head(resultC)
```

## Case Study

完整的模型建立步驟範例：

- 標題：以聲波撞擊礦石的回聲預測礦石是否為礦物
- 以Sonar, Mines vs. Rocks為例

**步驟1.1:讀資料**

```{r message=F,warning=F}
#install.packages("mlbench") # 此package內有很多dataset可練習
library(mlbench)
data(Sonar)
str(Sonar) #看一下資料型別，有沒有缺值，類別變項是不是factor
```

在建立模型之前...別忘了基本的資料分析，使用`探索性分析 Exploratory data analysis`，看看資料長怎麼樣，要是有一個參數可以完美的把礦物跟石頭分開，那就不用麻煩建模了...

探索性分析 Exploratory data analysis
```{r warning=F,message=F,fig.height=4}
library(ggplot2);library(reshape2) #install.packages(c("ggplot2","reshape2"))
Sonar.m<-melt(Sonar,id.vars = c("Class"))
ggplot(Sonar.m)+geom_boxplot(aes(x=Class,y=value))+
    facet_wrap(~variable, nrow=5,scales = "free_y") #圖片太小了
```

**步驟1.2: 資料前處理**
 
- 缺值？
    - 沒有缺值，不需要處理
- 答案種類？
    - 類別變項叫`Class`，M: mine礦-->+, R: rock-->-，不需要處理
- 類別變項的型別是不是factor？
    - 是，不需要處理
- 有沒有無關的參數？
    - 沒有無關的參數，不需要處理

**步驟2:分成訓練組與測試組**

該怎麼分可以自己決定，1/3，1/5...都可以
```{r}
Sonar$Test<-F #新增一個參數紀錄分組
#隨機取1/3當Test set
Sonar[sample(1:nrow(Sonar),nrow(Sonar)/3),"Test"]<-T
# 看一下 Training set : Test set 案例數
c(sum(Sonar$Test==F),sum(Sonar$Test==T))
```

**步驟3:訓練模型**

- 注意只能用`訓練組`的資料，`Test`參數==F，忘記可以看前面範例
- 數值自變項X很多，先用迴歸好了～
- 要解釋一下模型
```{r warning=F,message=F}
fit<-glm(Class~., Sonar[Sonar$Test==F,],family="binomial")
finalFit<-stepAIC(fit,direction = "both",trace = F)
summary(finalFit)$coefficients
```

**步驟4.1:用測試組驗證模型-預測**

```{r warning=F,message=F,fig.height=4.5}
MinePred<-predict(finalFit,newdata = Sonar[Sonar$Test==T,])
MineAns<-ifelse(MinePred>0.5,"R","M") #>0.5: Level 2
MineAns<-factor(MineAns,levels = c("M","R"))
MineAns
```

**步驟4.2:用測試組驗證模型-效能**

```{r warning=F,message=F,fig.height=4.5}
library(caret) # install.packages("caret") #計算參數的packages
sensitivity(MineAns,Sonar[Sonar$Test==T,]$Class)
specificity(MineAns,Sonar[Sonar$Test==T,]$Class)
posPredValue(MineAns,Sonar[Sonar$Test==T,]$Class)
negPredValue(MineAns,Sonar[Sonar$Test==T,]$Class)
```

**解釋範例 - 資料說明**

此資料來源為UCI Machine Learning Repository。

記載礦物與石頭接受各個不同角度的聲波撞擊後，接收到的回聲數值，一共有60個參數，代表使用一特別角度的聲波撞擊礦石所得回聲。另外，分類結果為二元分類，包括礦物 (M) 與石頭 (R) 。


**解釋範例 - 模型說明**

使用聲波在不同角度撞擊`礦石`所得到的回聲資料，以邏輯迴歸建立模型預測礦石是否為礦物，經最佳化後，模型使用參數為V1 + V2 + V3 + V4 + V7 + V11 + V12 + V13 + V17 + V18 + V22 + V24 + V25 + V26 + V30 + V31 + V32 + V38 + V39 + V48 + V50 + V52 + V53 + V58 + V59，共25個參數，各參數代表從一特別角度所得的礦石回聲

**解釋範例 - 預測效能說明**
 
使用聲波在不同角度撞擊`礦石`所得到的回聲資料，以邏輯迴歸模型預測礦石是否為礦物，可得敏感度97%，特異性89%，陽性預測率89%，陰性預測率97%。

## 參考資料
- 台大資工林軒田教授：
    - [Machine Learning Foundations](www.coursera.org/course/ntumlone)
    - [Machine Learning Techniques](www.coursera.org/course/ntumltwo)

- [Market Basket Analysis with R](http://www.salemmarafi.com/code/market-basket-analysis-with-r/)

- [Deep Learning in R](https://www.r-bloggers.com/deep-learning-in-r-2/)

